{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# To add a new cell, type ''\n",
        "# To add a new markdown cell, type ''\n",
        "\n",
        "\n",
        "# # Libraries\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from typing import Tuple, Dict, Union\n",
        "import warnings\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import tokenizers\n",
        "from transformers import RobertaModel, RobertaTokenizer\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "# # Seed\n",
        "\n",
        "\n",
        "def seed_everything(seed_value):\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed_value)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed_value)\n",
        "        torch.cuda.manual_seed_all(seed_value)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "\n",
        "\n",
        "# # Data Loader\n",
        "\n",
        "\n",
        "def _get_input_data(\n",
        "    row: pd.Series, tokenizer: tokenizers.ByteLevelBPETokenizer, max_len: int\n",
        ") -> Tuple[torch.tensor, torch.tensor, str, torch.tensor]:\n",
        "    tweet = \" \" + \" \".join(row.text.lower().split())\n",
        "    encoding = tokenizer.encode(tweet)\n",
        "    sentiment_id = tokenizer.encode(row.sentiment).ids\n",
        "    # <s> sentiment </s></s> encoding </s>\n",
        "    ids = (\n",
        "        [tokenizer.bos_token_id]\n",
        "        + sentiment_id\n",
        "        + [tokenizer.eos_token_id, tokenizer.eos_token_id]\n",
        "        + encoding.ids\n",
        "        + [tokenizer.eos_token_id]\n",
        "    )\n",
        "    offsets = [(0, 0)] * 4 + encoding.offsets + [(0, 0)]\n",
        "\n",
        "    pad_len = max_len - len(ids)\n",
        "    if pad_len > 0:\n",
        "        ids += [1] * pad_len\n",
        "        offsets += [(0, 0)] * pad_len\n",
        "\n",
        "    ids = torch.tensor(ids)\n",
        "    masks = torch.where(ids != 1, torch.tensor(1), torch.tensor(0))\n",
        "    offsets = torch.tensor(offsets)\n",
        "\n",
        "    assert ids.shape == torch.Size([96])\n",
        "    assert masks.shape == torch.Size([96])\n",
        "    assert offsets.shape == torch.Size([96, 2])\n",
        "\n",
        "    return ids, masks, tweet, offsets\n",
        "\n",
        "\n",
        "\n",
        "def _get_target_idx(row, tweet, offsets):\n",
        "    selected_text = \" \" + \" \".join(row.selected_text.lower().split())\n",
        "\n",
        "    len_st = len(selected_text) - 1\n",
        "    idx0 = None\n",
        "    idx1 = None\n",
        "\n",
        "    for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n",
        "        if \" \" + tweet[ind : ind + len_st] == selected_text:\n",
        "            idx0 = ind\n",
        "            idx1 = ind + len_st - 1\n",
        "            break\n",
        "\n",
        "    char_targets = [0] * len(tweet)\n",
        "    if idx0 is not None and idx1 is not None:\n",
        "        for ct in range(idx0, idx1 + 1):\n",
        "            char_targets[ct] = 1\n",
        "\n",
        "    target_idx = []\n",
        "    for j, (offset1, offset2) in enumerate(offsets):\n",
        "        if sum(char_targets[offset1:offset2]) > 0:\n",
        "            target_idx.append(j)\n",
        "\n",
        "    start_idx = target_idx[0]\n",
        "    end_idx = target_idx[-1]\n",
        "\n",
        "    return start_idx, end_idx\n",
        "\n",
        "\n",
        "\n",
        "def _process_row(\n",
        "    row: pd.Series, tokenizer: tokenizers.ByteLevelBPETokenizer, max_len: int\n",
        ") -> Dict[str, Union[torch.tensor, str, int]]:\n",
        "    \"\"\"\n",
        "    Process row so it can be fed into Roberta.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    row :\n",
        "        Observation (either training or testing)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dictionary that can be processed by training loop.\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    >>> row = pd.Series({'textID': 'cb774db0d1', 'text': ' I`d have responded, if I were going', 'selected_text': 'I`d have responded, if I were going', 'sentiment': 'neutral'})\n",
        "    >>> data = _process_row(row)\n",
        "    >>> data['tweet']\n",
        "    ' i`d have responded, if i were going'' i`d have responded, if i were going'\n",
        "\n",
        "    >>> data['ids'][data['masks']==1]\n",
        "    tensor([    0,  7974,     2,     2,   939, 12905,   417,    33,  2334,     6,\n",
        "            114,   939,    58,   164,     2])\n",
        "\n",
        "    >>> left, right = data['offsets'][7]\n",
        "    >>> data['tweet'][left:right]\n",
        "    ' have'\n",
        "\n",
        "    >>> [data['tweet'][data['offsets'][n][0]:data['offsets'][n][1]] for n in range(data['start_idx'], data['end_idx']+1)]\n",
        "    [' i', '`', 'd', ' have', ' responded', ',', ' if', ' i', ' were', ' going']\n",
        "    \"\"\"\n",
        "    data = {}\n",
        "\n",
        "    ids, masks, tweet, offsets = _get_input_data(row, tokenizer, max_len)\n",
        "    data[\"ids\"] = ids\n",
        "    data[\"masks\"] = masks\n",
        "    data[\"tweet\"] = tweet\n",
        "    data[\"offsets\"] = offsets\n",
        "\n",
        "    if \"selected_text\" in data:\n",
        "        start_idx, end_idx = _get_target_idx(row, tweet, offsets)\n",
        "        data[\"start_idx\"] = start_idx\n",
        "        data[\"end_idx\"] = end_idx\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "class TweetDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df, max_len=96):\n",
        "        self.df = df\n",
        "        self.max_len = max_len\n",
        "\n",
        "        tokenizer = RobertaTokenizer.from_pretrained(\"../input/roberta-base/\")\n",
        "        tokenizer.save_vocabulary(\".\")\n",
        "        for i in [\"bos\", \"cls\", \"eos\"]:\n",
        "            attribute = f\"{i}_token_id\"\n",
        "            setattr(self, attribute, getattr(tokenizer, attribute))\n",
        "\n",
        "        # vocab.json and merges.txt come from save_vocabulary above\n",
        "        self.tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
        "            vocab_file=\"./vocab.json\",\n",
        "            merges_file=\"./merges.txt\",\n",
        "            lowercase=True,\n",
        "            add_prefix_space=True,\n",
        "        )\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = self.df.iloc[index]\n",
        "\n",
        "        data = _process_row(row, self.tokenizer, self.max_len)\n",
        "\n",
        "        return data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "\n",
        "def get_train_val_loaders(df, train_idx, val_idx, batch_size=8):\n",
        "    train_df = df.iloc[train_idx]\n",
        "    val_df = df.iloc[val_idx]\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        TweetDataset(train_df),\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        drop_last=True,\n",
        "    )\n",
        "\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        TweetDataset(val_df),\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "    )\n",
        "\n",
        "    dataloaders_dict = {\"train\": train_loader, \"val\": val_loader}\n",
        "\n",
        "    return dataloaders_dict\n",
        "\n",
        "\n",
        "def get_test_loader(df, batch_size=32):\n",
        "    loader = torch.utils.data.DataLoader(\n",
        "        TweetDataset(df), batch_size=batch_size, shuffle=False, num_workers=2\n",
        "    )\n",
        "    return loader\n",
        "\n",
        "\n",
        "# # Model\n",
        "\n",
        "\n",
        "class TweetModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TweetModel, self).__init__()\n",
        "\n",
        "        self.roberta = RobertaModel.from_pretrained(\n",
        "            \"../input/roberta-base\", output_hidden_states=True\n",
        "        )\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc = nn.Linear(self.roberta.config.hidden_size, 2)\n",
        "        nn.init.normal_(self.fc.weight, std=0.02)\n",
        "        nn.init.normal_(self.fc.bias, 0)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        _, _, hs = self.roberta(input_ids, attention_mask)\n",
        "\n",
        "        x = torch.stack([hs[-1], hs[-2], hs[-3], hs[-4]])\n",
        "        x = torch.mean(x, 0)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        start_logits, end_logits = x.split(1, dim=-1)\n",
        "        start_logits = start_logits.squeeze(-1)\n",
        "        end_logits = end_logits.squeeze(-1)\n",
        "\n",
        "        return start_logits, end_logits\n",
        "\n",
        "\n",
        "# # Loss Function\n",
        "\n",
        "\n",
        "def loss_fn(start_logits, end_logits, start_positions, end_positions):\n",
        "    ce_loss = nn.CrossEntropyLoss()\n",
        "    start_loss = ce_loss(start_logits, start_positions)\n",
        "    end_loss = ce_loss(end_logits, end_positions)\n",
        "    total_loss = start_loss + end_loss\n",
        "    return total_loss\n",
        "\n",
        "\n",
        "# # Evaluation Function\n",
        "\n",
        "\n",
        "def get_selected_text(text, start_idx, end_idx, offsets):\n",
        "    selected_text = \"\"\n",
        "    for ix in range(start_idx, end_idx + 1):\n",
        "        selected_text += text[offsets[ix][0] : offsets[ix][1]]\n",
        "        if (ix + 1) < len(offsets) and offsets[ix][1] < offsets[ix + 1][0]:\n",
        "            selected_text += \" \"\n",
        "    return selected_text\n",
        "\n",
        "\n",
        "def jaccard(str1, str2):\n",
        "    a = set(str1.lower().split())\n",
        "    b = set(str2.lower().split())\n",
        "    c = a.intersection(b)\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
        "\n",
        "\n",
        "def compute_jaccard_score(\n",
        "    text, start_idx, end_idx, start_logits, end_logits, offsets\n",
        "):\n",
        "    start_pred = np.argmax(start_logits)\n",
        "    end_pred = np.argmax(end_logits)\n",
        "    if start_pred > end_pred:\n",
        "        pred = text\n",
        "    else:\n",
        "        pred = get_selected_text(text, start_pred, end_pred, offsets)\n",
        "\n",
        "    true = get_selected_text(text, start_idx, end_idx, offsets)\n",
        "\n",
        "    return jaccard(true, pred)\n",
        "\n",
        "\n",
        "# # Training Function\n",
        "\n",
        "\n",
        "def loop_through_data_loader(\n",
        "    model, dataloaders_dict, criterion, optimizer, num_epochs, epoch, phase\n",
        "):\n",
        "    epoch_loss = 0.0\n",
        "    epoch_jaccard = 0.0\n",
        "\n",
        "    for data in dataloaders_dict[phase]:\n",
        "        ids = data[\"ids\"].cuda()\n",
        "        masks = data[\"masks\"].cuda()\n",
        "        tweet = data[\"tweet\"]\n",
        "        offsets = data[\"offsets\"].numpy()\n",
        "        start_idx = data[\"start_idx\"].cuda()\n",
        "        end_idx = data[\"end_idx\"].cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.set_grad_enabled(phase == \"train\"):\n",
        "\n",
        "            start_logits, end_logits = model(ids, masks)\n",
        "\n",
        "            loss = criterion(start_logits, end_logits, start_idx, end_idx)\n",
        "\n",
        "            if phase == \"train\":\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item() * len(ids)\n",
        "\n",
        "            start_idx = start_idx.cpu().detach().numpy()\n",
        "            end_idx = end_idx.cpu().detach().numpy()\n",
        "            start_logits = (\n",
        "                torch.softmax(start_logits, dim=1).cpu().detach().numpy()\n",
        "            )\n",
        "            end_logits = (\n",
        "                torch.softmax(end_logits, dim=1).cpu().detach().numpy()\n",
        "            )\n",
        "\n",
        "            for i in range(len(ids)):\n",
        "                jaccard_score = compute_jaccard_score(\n",
        "                    tweet[i],\n",
        "                    start_idx[i],\n",
        "                    end_idx[i],\n",
        "                    start_logits[i],\n",
        "                    end_logits[i],\n",
        "                    offsets[i],\n",
        "                )\n",
        "                epoch_jaccard += jaccard_score\n",
        "\n",
        "    epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
        "    epoch_jaccard = epoch_jaccard / len(dataloaders_dict[phase].dataset)\n",
        "\n",
        "    print(\n",
        "        \"Epoch {}/{} | {:^5} | Loss: {:.4f} | Jaccard: {:.4f}\".format(\n",
        "            epoch + 1, num_epochs, phase, epoch_loss, epoch_jaccard\n",
        "        )\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def train_one_epoch(\n",
        "    model, dataloaders_dict, criterion, optimizer, num_epochs, epoch\n",
        "):\n",
        "    # train\n",
        "    model.train()\n",
        "    model = loop_through_data_loader(\n",
        "        model,\n",
        "        dataloaders_dict,\n",
        "        criterion,\n",
        "        optimizer,\n",
        "        num_epochs,\n",
        "        epoch,\n",
        "        \"train\",\n",
        "    )\n",
        "\n",
        "    # evaluate\n",
        "    model.eval()\n",
        "    model = loop_through_data_loader(\n",
        "        model, dataloaders_dict, criterion, optimizer, num_epochs, epoch, \"val\"\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def train_model(\n",
        "    model, dataloaders_dict, criterion, optimizer, num_epochs, filename\n",
        "):\n",
        "    model.cuda()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model = train_one_epoch(\n",
        "            model, dataloaders_dict, criterion, optimizer, num_epochs, epoch\n",
        "        )\n",
        "\n",
        "    torch.save(model.state_dict(), filename)\n",
        "\n",
        "\n",
        "# # Training\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    seed = 42\n",
        "    seed_everything(seed)\n",
        "\n",
        "    num_epochs = 3\n",
        "    batch_size = 32\n",
        "    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "\n",
        "    train_df = pd.read_csv(\"../input/tweet-sentiment-extraction/train.csv\")\n",
        "    train_df[\"text\"] = train_df[\"text\"].astype(str)\n",
        "    train_df[\"selected_text\"] = train_df[\"selected_text\"].astype(str)\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(\n",
        "        skf.split(train_df, train_df.sentiment), start=1\n",
        "    ):\n",
        "        print(f\"Fold: {fold}\")\n",
        "\n",
        "        model = TweetModel()\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=3e-5, betas=(0.9, 0.999))\n",
        "        criterion = loss_fn\n",
        "        dataloaders_dict = get_train_val_loaders(\n",
        "            train_df, train_idx, val_idx, batch_size\n",
        "        )\n",
        "\n",
        "        train_model(\n",
        "            model,\n",
        "            dataloaders_dict,\n",
        "            criterion,\n",
        "            optimizer,\n",
        "            num_epochs,\n",
        "            f\"roberta_fold{fold}.pth\",\n",
        "        )\n",
        "\n",
        "    # # Inference\n",
        "\n",
        "    test_df = pd.read_csv(\"../input/tweet-sentiment-extraction/test.csv\")\n",
        "    test_df[\"text\"] = test_df[\"text\"].astype(str)\n",
        "    test_loader = get_test_loader(test_df)\n",
        "    predictions = []\n",
        "    models = []\n",
        "    for fold in range(skf.n_splits):\n",
        "        model = TweetModel()\n",
        "        model.cuda()\n",
        "        model.load_state_dict(torch.load(f\"roberta_fold{fold+1}.pth\"))\n",
        "        model.eval()\n",
        "        models.append(model)\n",
        "\n",
        "    for data in test_loader:\n",
        "        ids = data[\"ids\"].cuda()\n",
        "        masks = data[\"masks\"].cuda()\n",
        "        tweet = data[\"tweet\"]\n",
        "        offsets = data[\"offsets\"].numpy()\n",
        "\n",
        "        start_logits = []\n",
        "        end_logits = []\n",
        "        for model in models:\n",
        "            with torch.no_grad():\n",
        "                output = model(ids, masks)\n",
        "                start_logits.append(\n",
        "                    torch.softmax(output[0], dim=1).cpu().detach().numpy()\n",
        "                )\n",
        "                end_logits.append(\n",
        "                    torch.softmax(output[1], dim=1).cpu().detach().numpy()\n",
        "                )\n",
        "\n",
        "        start_logits = np.mean(start_logits, axis=0)\n",
        "        end_logits = np.mean(end_logits, axis=0)\n",
        "        for i in range(len(ids)):\n",
        "            start_pred = np.argmax(start_logits[i])\n",
        "            end_pred = np.argmax(end_logits[i])\n",
        "            if start_pred > end_pred:\n",
        "                pred = tweet[i]\n",
        "            else:\n",
        "                pred = get_selected_text(\n",
        "                    tweet[i], start_pred, end_pred, offsets[i]\n",
        "                )\n",
        "            predictions.append(pred)\n",
        "\n",
        "    # # Submission\n",
        "\n",
        "    sub_df = pd.read_csv(\n",
        "        \"../input/tweet-sentiment-extraction/sample_submission.csv\"\n",
        "    )\n",
        "    sub_df[\"selected_text\"] = predictions\n",
        "    sub_df[\"selected_text\"] = sub_df[\"selected_text\"].apply(\n",
        "        lambda x: x.replace(\"!!!!\", \"!\") if len(x.split()) == 1 else x\n",
        "    )\n",
        "    sub_df[\"selected_text\"] = sub_df[\"selected_text\"].apply(\n",
        "        lambda x: x.replace(\"..\", \".\") if len(x.split()) == 1 else x\n",
        "    )\n",
        "    sub_df[\"selected_text\"] = sub_df[\"selected_text\"].apply(\n",
        "        lambda x: x.replace(\"...\", \".\") if len(x.split()) == 1 else x\n",
        "    )\n",
        "    sub_df.to_csv(\"submission.csv\", index=False)\n",
        "    sub_df.head()\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
