{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tokenizers\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    from transformers import RobertaModel, RobertaTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMOKE_TEST = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed_value)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "seed = 42\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_input_data(\n",
    "    row: pd.Series,\n",
    "    tokenizer: tokenizers.ByteLevelBPETokenizer,\n",
    "    max_len: int,\n",
    "    bos_token_id: int,\n",
    "    eos_token_id: int,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Encode input row so it can be processed by Roberta.\n",
    "\n",
    "    Encoding follows: <s> sentiment </s></s> encoding </s>\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    row :\n",
    "        Raw data\n",
    "    tokenizer :\n",
    "        Converts text to stream of tokens\n",
    "    max_len :\n",
    "        Maximum length of encoded streams\n",
    "    bos_token_id :\n",
    "        Beginning of sentence token\n",
    "    eos_token_id :\n",
    "        End of sentence token\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ids :\n",
    "        Encoded stream of text\n",
    "    masks :\n",
    "        1 for tokens corresponding to <s> sentiment </s></s> encoding </s>,\n",
    "        0 for tokens corresponding to padding\n",
    "    tweet :\n",
    "        Raw tweet\n",
    "    offsets :\n",
    "        Positions of raw text corresponding to each token\n",
    "    \"\"\"\n",
    "    tweet = \" \" + \" \".join(row.text.lower().split())\n",
    "    encoding = tokenizer.encode(tweet)\n",
    "    sentiment_id = tokenizer.encode(row.sentiment).ids\n",
    "    ids = (\n",
    "        [bos_token_id]\n",
    "        + sentiment_id\n",
    "        + [eos_token_id, eos_token_id]\n",
    "        + encoding.ids\n",
    "        + [eos_token_id]\n",
    "    )\n",
    "    offsets = [(0, 0)] * 4 + encoding.offsets + [(0, 0)]\n",
    "\n",
    "    pad_len = max_len - len(ids)\n",
    "    if pad_len > 0:\n",
    "        ids += [1] * pad_len\n",
    "        offsets += [(0, 0)] * pad_len\n",
    "\n",
    "    ids = torch.tensor(ids)\n",
    "    masks = torch.where(ids != 1, torch.tensor(1), torch.tensor(0))\n",
    "    offsets = torch.tensor(offsets)\n",
    "\n",
    "    assert ids.shape == torch.Size([96])\n",
    "    assert masks.shape == torch.Size([96])\n",
    "    assert offsets.shape == torch.Size([96, 2])\n",
    "\n",
    "    return ids, masks, tweet, offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_target_idx(row: pd.Series, tweet: str, offsets: torch.Tensor) -> Tuple[int, int]:\n",
    "\n",
    "    assert offsets.shape == torch.Size([96, 2])\n",
    "\n",
    "    selected_text = \" \" + \" \".join(row.selected_text.lower().split())\n",
    "\n",
    "    len_st = len(selected_text) - 1\n",
    "    idx0 = None\n",
    "    idx1 = None\n",
    "\n",
    "    for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n",
    "        if \" \" + tweet[ind : ind + len_st] == selected_text:\n",
    "            idx0 = ind\n",
    "            idx1 = ind + len_st - 1\n",
    "            break\n",
    "\n",
    "    char_targets = [0] * len(tweet)\n",
    "    if idx0 is not None and idx1 is not None:\n",
    "        for ct in range(idx0, idx1 + 1):\n",
    "            char_targets[ct] = 1\n",
    "\n",
    "    target_idx: List[int] = []\n",
    "    for j, (offset1, offset2) in enumerate(offsets):\n",
    "        left = offset1.item()\n",
    "        right = offset2.item()\n",
    "        assert isinstance(left, int)\n",
    "        assert isinstance(right, int)\n",
    "        if sum(char_targets[left:right]) > 0:\n",
    "            target_idx.append(j)\n",
    "\n",
    "    start_idx = target_idx[0]\n",
    "    end_idx = target_idx[-1]\n",
    "\n",
    "    return start_idx, end_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_row(\n",
    "    row: pd.Series,\n",
    "    tokenizer: tokenizers.ByteLevelBPETokenizer,\n",
    "    max_len: int,\n",
    "    bos_token_id: int,\n",
    "    eos_token_id: int,\n",
    ") -> Dict[str, Union[torch.Tensor, str, int]]:\n",
    "    \"\"\"\n",
    "    Process row so it can be fed into Roberta.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    row :\n",
    "        Raw data\n",
    "    tokenizer :\n",
    "        Converts text to stream of tokens\n",
    "    max_len :\n",
    "        Maximum length of encoded streams\n",
    "    bos_token_id :\n",
    "        Beginning of sentence token\n",
    "    eos_token_id :\n",
    "        End of sentence token\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dictionary that can be processed by training loop.\n",
    "    \"\"\"\n",
    "    data: Dict[str, Union[torch.Tensor, str, int]] = {}\n",
    "\n",
    "    ids, masks, tweet, offsets = _get_input_data(\n",
    "        row, tokenizer, max_len, bos_token_id, eos_token_id\n",
    "    )\n",
    "    data.update({\"ids\": ids})\n",
    "    data[\"masks\"] = masks\n",
    "    data[\"tweet\"] = tweet\n",
    "    data[\"offsets\"] = offsets\n",
    "\n",
    "    if \"selected_text\" in row:\n",
    "        start_idx, end_idx = _get_target_idx(row, tweet, offsets)\n",
    "        data[\"start_idx\"] = start_idx\n",
    "        data[\"end_idx\"] = end_idx\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = pd.Series(\n",
    "    {\n",
    "        \"textID\": \"cb774db0d1\",\n",
    "        \"text\": \" I`d have responded, if I were going\",\n",
    "        \"selected_text\": \"I`d have responded, if I were going\",\n",
    "        \"sentiment\": \"neutral\",\n",
    "    }\n",
    ")\n",
    "r_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "r_tokenizer.save_vocabulary(\".\")\n",
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "    vocab_file=\"./vocab.json\",\n",
    "    merges_file=\"./merges.txt\",\n",
    "    lowercase=True,\n",
    "    add_prefix_space=True,\n",
    ")\n",
    "\n",
    "\n",
    "def test_process_row():\n",
    "    data = _process_row(row, tokenizer, 96, r_tokenizer.bos_token_id, r_tokenizer.eos_token_id)\n",
    "    data[\"tweet\"]\n",
    "    result_ids = data[\"ids\"][data[\"masks\"] == 1]\n",
    "\n",
    "    expected_ids = torch.tensor(\n",
    "        [0, 7974, 2, 2, 939, 12905, 417, 33, 2334, 6, 114, 939, 58, 164, 2]\n",
    "    )\n",
    "    torch.all(result_ids.eq(expected_ids))\n",
    "    result_selected_text = [\n",
    "        data[\"tweet\"][data[\"offsets\"][n][0] : data[\"offsets\"][n][1]]\n",
    "        for n in range(data[\"start_idx\"], data[\"end_idx\"] + 1)\n",
    "    ]\n",
    "    expected_selected_text = [\n",
    "        \" i\",\n",
    "        \"`\",\n",
    "        \"d\",\n",
    "        \" have\",\n",
    "        \" responded\",\n",
    "        \",\",\n",
    "        \" if\",\n",
    "        \" i\",\n",
    "        \" were\",\n",
    "        \" going\",\n",
    "    ]\n",
    "    assert result_selected_text == expected_selected_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, max_len: int = 96) -> None:\n",
    "        self.df = df\n",
    "        self.max_len = max_len\n",
    "        self.labeled = \"selected_text\" in df\n",
    "\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(\"../input/roberta-base/\")\n",
    "        tokenizer.save_vocabulary(\".\")\n",
    "        self.bos_token_id = tokenizer.bos_token_id\n",
    "        self.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "        # vocab.json and merges.txt come from save_vocabulary above\n",
    "        self.tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "            vocab_file=\"./vocab.json\",\n",
    "            merges_file=\"./merges.txt\",\n",
    "            lowercase=True,\n",
    "            add_prefix_space=True,\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, index: int) -> Dict[str, Union[torch.Tensor, str, int]]:\n",
    "        row = self.df.iloc[index]\n",
    "        data = _process_row(\n",
    "            row, self.tokenizer, self.max_len, self.bos_token_id, self.eos_token_id\n",
    "        )\n",
    "        return data\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "\n",
    "\n",
    "def get_train_val_loaders(df, train_idx, val_idx, batch_size=8):\n",
    "    train_df = df.iloc[train_idx]\n",
    "    val_df = df.iloc[val_idx]\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        TweetDataset(train_df),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        TweetDataset(val_df), batch_size=batch_size, shuffle=False, num_workers=2,\n",
    "    )\n",
    "\n",
    "    dataloaders_dict = {\"train\": train_loader, \"val\": val_loader}\n",
    "\n",
    "    return dataloaders_dict\n",
    "\n",
    "\n",
    "def get_test_loader(df, batch_size=32):\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        TweetDataset(df), batch_size=batch_size, shuffle=False, num_workers=2\n",
    "    )\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TweetModel, self).__init__()\n",
    "\n",
    "        self.roberta = RobertaModel.from_pretrained(\n",
    "            \"../input/roberta-base\", output_hidden_states=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(self.roberta.config.hidden_size, 2)\n",
    "        nn.init.normal_(self.fc.weight, std=0.02)\n",
    "        nn.init.normal_(self.fc.bias, 0)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, _, hs = self.roberta(input_ids, attention_mask)\n",
    "\n",
    "        x = torch.stack([hs[-1], hs[-2], hs[-3], hs[-4]])\n",
    "        x = torch.mean(x, 0)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        start_logits, end_logits = x.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "\n",
    "        return start_logits, end_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(start_logits, end_logits, start_positions, end_positions):\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    start_loss = ce_loss(start_logits, start_positions)\n",
    "    end_loss = ce_loss(end_logits, end_positions)\n",
    "    total_loss = start_loss + end_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_selected_text(text, start_idx, end_idx, offsets):\n",
    "    selected_text = \"\"\n",
    "    for ix in range(start_idx, end_idx + 1):\n",
    "        selected_text += text[offsets[ix][0] : offsets[ix][1]]\n",
    "        if (ix + 1) < len(offsets) and offsets[ix][1] < offsets[ix + 1][0]:\n",
    "            selected_text += \" \"\n",
    "    return selected_text\n",
    "\n",
    "\n",
    "def jaccard(str1, str2):\n",
    "    a = set(str1.lower().split())\n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "\n",
    "def compute_jaccard_score(text, start_idx, end_idx, start_logits, end_logits, offsets):\n",
    "    start_pred = np.argmax(start_logits)\n",
    "    end_pred = np.argmax(end_logits)\n",
    "    if start_pred > end_pred:\n",
    "        pred = text\n",
    "    else:\n",
    "        pred = get_selected_text(text, start_pred, end_pred, offsets)\n",
    "\n",
    "    true = get_selected_text(text, start_idx, end_idx, offsets)\n",
    "\n",
    "    return jaccard(true, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_through_data_loader(\n",
    "    model, dataloaders_dict, criterion, optimizer, num_epochs, epoch, phase\n",
    "):\n",
    "    epoch_loss = 0.0\n",
    "    epoch_jaccard = 0.0\n",
    "\n",
    "    for data in dataloaders_dict[phase]:\n",
    "        ids = data[\"ids\"].cuda()\n",
    "        masks = data[\"masks\"].cuda()\n",
    "        tweet = data[\"tweet\"]\n",
    "        offsets = data[\"offsets\"].numpy()\n",
    "        start_idx = data[\"start_idx\"].cuda()\n",
    "        end_idx = data[\"end_idx\"].cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.set_grad_enabled(phase == \"train\"):\n",
    "\n",
    "            start_logits, end_logits = model(ids, masks)\n",
    "\n",
    "            loss = criterion(start_logits, end_logits, start_idx, end_idx)\n",
    "\n",
    "            if phase == \"train\":\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item() * len(ids)\n",
    "\n",
    "            start_idx = start_idx.cpu().detach().numpy()\n",
    "            end_idx = end_idx.cpu().detach().numpy()\n",
    "            start_logits = torch.softmax(start_logits, dim=1).cpu().detach().numpy()\n",
    "            end_logits = torch.softmax(end_logits, dim=1).cpu().detach().numpy()\n",
    "\n",
    "            for i in range(len(ids)):\n",
    "                jaccard_score = compute_jaccard_score(\n",
    "                    tweet[i],\n",
    "                    start_idx[i],\n",
    "                    end_idx[i],\n",
    "                    start_logits[i],\n",
    "                    end_logits[i],\n",
    "                    offsets[i],\n",
    "                )\n",
    "                epoch_jaccard += jaccard_score\n",
    "\n",
    "    epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
    "    epoch_jaccard = epoch_jaccard / len(dataloaders_dict[phase].dataset)\n",
    "\n",
    "    print(\n",
    "        \"Epoch {}/{} | {:^5} | Loss: {:.4f} | Jaccard: {:.4f}\".format(\n",
    "            epoch + 1, num_epochs, phase, epoch_loss, epoch_jaccard\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloaders_dict, criterion, optimizer, num_epochs, epoch):\n",
    "    # train\n",
    "    model.train()\n",
    "    model = loop_through_data_loader(\n",
    "        model, dataloaders_dict, criterion, optimizer, num_epochs, epoch, \"train\",\n",
    "    )\n",
    "\n",
    "    # evaluate\n",
    "    model.eval()\n",
    "    model = loop_through_data_loader(\n",
    "        model, dataloaders_dict, criterion, optimizer, num_epochs, epoch, \"val\"\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders_dict, criterion, optimizer, num_epochs, filename):\n",
    "    model.cuda()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model = train_one_epoch(\n",
    "            model, dataloaders_dict, criterion, optimizer, num_epochs, epoch\n",
    "        )\n",
    "\n",
    "    torch.save(model.state_dict(), filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SMOKE_TEST:\n",
    "    num_epochs = 1\n",
    "else:\n",
    "    num_epochs = 3\n",
    "batch_size = 32\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "Epoch 1/3 | train | Loss: 2.1321 | Jaccard: 0.6634\n",
      "Epoch 1/3 |  val  | Loss: 1.6480 | Jaccard: 0.7166\n",
      "Epoch 2/3 | train | Loss: 1.6013 | Jaccard: 0.7182\n",
      "Epoch 2/3 |  val  | Loss: 1.6582 | Jaccard: 0.7059\n",
      "Epoch 3/3 | train | Loss: 1.4530 | Jaccard: 0.7373\n",
      "Epoch 3/3 |  val  | Loss: 1.7208 | Jaccard: 0.7131\n",
      "Fold: 2\n",
      "Epoch 1/3 | train | Loss: 2.1968 | Jaccard: 0.6553\n",
      "Epoch 1/3 |  val  | Loss: 1.7515 | Jaccard: 0.6925\n",
      "Epoch 2/3 | train | Loss: 1.6998 | Jaccard: 0.7068\n",
      "Epoch 2/3 |  val  | Loss: 1.6465 | Jaccard: 0.7124\n",
      "Epoch 3/3 | train | Loss: 1.5439 | Jaccard: 0.7252\n",
      "Epoch 3/3 |  val  | Loss: 1.6395 | Jaccard: 0.7160\n",
      "Fold: 3\n",
      "Epoch 1/3 | train | Loss: 2.1237 | Jaccard: 0.6675\n",
      "Epoch 1/3 |  val  | Loss: 1.7006 | Jaccard: 0.7078\n",
      "Epoch 2/3 | train | Loss: 1.6322 | Jaccard: 0.7172\n",
      "Epoch 2/3 |  val  | Loss: 1.6523 | Jaccard: 0.7141\n",
      "Epoch 3/3 | train | Loss: 1.4792 | Jaccard: 0.7337\n",
      "Epoch 3/3 |  val  | Loss: 1.6673 | Jaccard: 0.7169\n",
      "Fold: 4\n",
      "Epoch 1/3 | train | Loss: 2.1617 | Jaccard: 0.6601\n",
      "Epoch 1/3 |  val  | Loss: 1.5938 | Jaccard: 0.7212\n",
      "Epoch 2/3 | train | Loss: 1.6270 | Jaccard: 0.7123\n",
      "Epoch 2/3 |  val  | Loss: 1.5249 | Jaccard: 0.7308\n",
      "Epoch 3/3 | train | Loss: 1.4600 | Jaccard: 0.7373\n",
      "Epoch 3/3 |  val  | Loss: 1.5597 | Jaccard: 0.7247\n",
      "Fold: 5\n",
      "Epoch 1/3 | train | Loss: 2.0595 | Jaccard: 0.6713\n",
      "Epoch 1/3 |  val  | Loss: 1.6322 | Jaccard: 0.7195\n",
      "Epoch 2/3 | train | Loss: 1.5929 | Jaccard: 0.7210\n",
      "Epoch 2/3 |  val  | Loss: 1.6388 | Jaccard: 0.7158\n",
      "Epoch 3/3 | train | Loss: 1.5069 | Jaccard: 0.7328\n",
      "Epoch 3/3 |  val  | Loss: 1.6546 | Jaccard: 0.7208\n",
      "Fold: 6\n",
      "Epoch 1/3 | train | Loss: 2.0796 | Jaccard: 0.6708\n",
      "Epoch 1/3 |  val  | Loss: 1.6544 | Jaccard: 0.7078\n",
      "Epoch 2/3 | train | Loss: 1.5919 | Jaccard: 0.7180\n",
      "Epoch 2/3 |  val  | Loss: 1.6450 | Jaccard: 0.7173\n",
      "Epoch 3/3 | train | Loss: 1.4351 | Jaccard: 0.7392\n",
      "Epoch 3/3 |  val  | Loss: 1.6319 | Jaccard: 0.7206\n",
      "Fold: 7\n",
      "Epoch 1/3 | train | Loss: 2.1183 | Jaccard: 0.6655\n",
      "Epoch 1/3 |  val  | Loss: 1.5971 | Jaccard: 0.7187\n",
      "Epoch 2/3 | train | Loss: 1.6268 | Jaccard: 0.7120\n",
      "Epoch 2/3 |  val  | Loss: 1.5684 | Jaccard: 0.7219\n",
      "Epoch 3/3 | train | Loss: 1.4952 | Jaccard: 0.7317\n",
      "Epoch 3/3 |  val  | Loss: 1.5916 | Jaccard: 0.7231\n",
      "Fold: 8\n",
      "Epoch 1/3 | train | Loss: 2.1916 | Jaccard: 0.6603\n",
      "Epoch 1/3 |  val  | Loss: 1.6748 | Jaccard: 0.7050\n",
      "Epoch 2/3 | train | Loss: 1.6450 | Jaccard: 0.7120\n",
      "Epoch 2/3 |  val  | Loss: 1.6062 | Jaccard: 0.7196\n",
      "Epoch 3/3 | train | Loss: 1.5097 | Jaccard: 0.7304\n",
      "Epoch 3/3 |  val  | Loss: 1.6520 | Jaccard: 0.7165\n",
      "Fold: 9\n",
      "Epoch 1/3 | train | Loss: 2.1056 | Jaccard: 0.6662\n",
      "Epoch 1/3 |  val  | Loss: 1.7136 | Jaccard: 0.7122\n",
      "Epoch 2/3 | train | Loss: 1.6063 | Jaccard: 0.7147\n",
      "Epoch 2/3 |  val  | Loss: 1.6593 | Jaccard: 0.7206\n",
      "Epoch 3/3 | train | Loss: 1.4619 | Jaccard: 0.7339\n",
      "Epoch 3/3 |  val  | Loss: 1.6499 | Jaccard: 0.7212\n",
      "Fold: 10\n",
      "Epoch 1/3 | train | Loss: 2.0821 | Jaccard: 0.6699\n",
      "Epoch 1/3 |  val  | Loss: 1.6264 | Jaccard: 0.7104\n",
      "Epoch 2/3 | train | Loss: 1.5766 | Jaccard: 0.7236\n",
      "Epoch 2/3 |  val  | Loss: 1.6017 | Jaccard: 0.7184\n",
      "Epoch 3/3 | train | Loss: 1.4345 | Jaccard: 0.7424\n",
      "Epoch 3/3 |  val  | Loss: 1.6217 | Jaccard: 0.7186\n",
      "CPU times: user 1h 24min 42s, sys: 34min 58s, total: 1h 59min 40s\n",
      "Wall time: 2h 5min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    train_df = pd.read_csv(\"../input/tweet-sentiment-extraction/train.csv\")\n",
    "    train_df[\"text\"] = train_df[\"text\"].astype(str)\n",
    "    train_df[\"selected_text\"] = train_df[\"selected_text\"].astype(str)\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(\n",
    "        skf.split(train_df, train_df.sentiment), start=1\n",
    "    ):\n",
    "        print(f\"Fold: {fold}\")\n",
    "\n",
    "        model = TweetModel()\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=3e-5, betas=(0.9, 0.999))\n",
    "        criterion = loss_fn\n",
    "        dataloaders_dict = get_train_val_loaders(train_df, train_idx, val_idx, batch_size)\n",
    "\n",
    "        train_model(\n",
    "            model,\n",
    "            dataloaders_dict,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            num_epochs,\n",
    "            f\"roberta_fold{fold}.pth\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 50s, sys: 41.5 s, total: 2min 32s\n",
      "Wall time: 2min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_df = pd.read_csv(\"../input/tweet-sentiment-extraction/test.csv\")\n",
    "    test_df[\"text\"] = test_df[\"text\"].astype(str)\n",
    "    test_loader = get_test_loader(test_df)\n",
    "    predictions = []\n",
    "    models = []\n",
    "    for fold in range(skf.n_splits):\n",
    "        model = TweetModel()\n",
    "        model.cuda()\n",
    "        model.load_state_dict(torch.load(f\"roberta_fold{fold+1}.pth\"))\n",
    "        model.eval()\n",
    "        models.append(model)\n",
    "\n",
    "    for data in test_loader:\n",
    "        ids = data[\"ids\"].cuda()\n",
    "        masks = data[\"masks\"].cuda()\n",
    "        tweet = data[\"tweet\"]\n",
    "        offsets = data[\"offsets\"].numpy()\n",
    "\n",
    "        start_logits = []\n",
    "        end_logits = []\n",
    "        for model in models:\n",
    "            with torch.no_grad():\n",
    "                output = model(ids, masks)\n",
    "                start_logits.append(torch.softmax(output[0], dim=1).cpu().detach().numpy())\n",
    "                end_logits.append(torch.softmax(output[1], dim=1).cpu().detach().numpy())\n",
    "\n",
    "        start_logits = np.mean(start_logits, axis=0)\n",
    "        end_logits = np.mean(end_logits, axis=0)\n",
    "        for i in range(len(ids)):\n",
    "            start_pred = np.argmax(start_logits[i])\n",
    "            end_pred = np.argmax(end_logits[i])\n",
    "            if start_pred > end_pred:\n",
    "                pred = tweet[i]\n",
    "            else:\n",
    "                pred = get_selected_text(tweet[i], start_pred, end_pred, offsets[i])\n",
    "            predictions.append(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f87dea47db</td>\n",
       "      <td>last session of the day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96d74cb729</td>\n",
       "      <td>exciting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eee518ae67</td>\n",
       "      <td>such a shame!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01082688c6</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33987a8ee5</td>\n",
       "      <td>i like it!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID             selected_text\n",
       "0  f87dea47db   last session of the day\n",
       "1  96d74cb729                  exciting\n",
       "2  eee518ae67             such a shame!\n",
       "3  01082688c6                     happy\n",
       "4  33987a8ee5               i like it!!"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    sub_df = pd.read_csv(\"../input/tweet-sentiment-extraction/sample_submission.csv\")\n",
    "    sub_df[\"selected_text\"] = predictions\n",
    "    sub_df[\"selected_text\"] = sub_df[\"selected_text\"].apply(\n",
    "        lambda x: x.replace(\"!!!!\", \"!\") if len(x.split()) == 1 else x\n",
    "    )\n",
    "    sub_df[\"selected_text\"] = sub_df[\"selected_text\"].apply(\n",
    "        lambda x: x.replace(\"..\", \".\") if len(x.split()) == 1 else x\n",
    "    )\n",
    "    sub_df[\"selected_text\"] = sub_df[\"selected_text\"].apply(\n",
    "        lambda x: x.replace(\"...\", \".\") if len(x.split()) == 1 else x\n",
    "    )\n",
    "    sub_df.to_csv(\"submission.csv\", index=False)\n",
    "    sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
