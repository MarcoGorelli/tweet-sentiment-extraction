{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet sentiment extraction\n",
    "\n",
    "This notebook a cleaned up / refactored version of [this](https://www.kaggle.com/shoheiazuma/tweet-sentiment-roberta-pytorch) one.\n",
    "\n",
    "Type-checking, linting, and testing is done via [nbQA](https://github.com/MarcoGorelli/nbQA) , e.g.\n",
    "\n",
    "```\n",
    "nbqa mypy cln-tweet-sentiment-roberta-pytorch.ipynb\n",
    "```\n",
    "\n",
    "----\n",
    "\n",
    "Disclaimer: I'm the authour of `nbQA`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tokenizers\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    from transformers import RobertaModel, RobertaTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "source": [
    "RUN_LOCAL = os.environ.get(\"PWD\") != \"/kaggle/working\"\n",
    "RUN_LOCAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "if RUN_LOCAL:\n",
    "    NUM_EPOCHS = 1\n",
    "    HOME_DIR = \"/workspaces/kaggle\"\n",
    "    BATCH_SIZE = 1\n",
    "    SKF = StratifiedKFold(n_splits=2, shuffle=True, random_state=SEED)\n",
    "else:\n",
    "    NUM_EPOCHS = 3\n",
    "    HOME_DIR = \"/kaggle\"\n",
    "    BATCH_SIZE = 32\n",
    "    SKF = StratifiedKFold(n_splits=10, shuffle=True, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed_value: int) -> None:\n",
    "    \"\"\"\n",
    "    Make notebook results exactly reproducible.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    seed_value\n",
    "        Random seed.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed_value)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_input_data(\n",
    "    row: pd.Series,\n",
    "    tokenizer: tokenizers.ByteLevelBPETokenizer,\n",
    "    max_len: int,\n",
    "    bos_token_id: int,\n",
    "    eos_token_id: int,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Encode input row so it can be processed by Roberta.\n",
    "\n",
    "    Encoding follows: <s> sentiment </s></s> encoding </s>\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    row :\n",
    "        Raw data\n",
    "    tokenizer :\n",
    "        Converts text to stream of tokens\n",
    "    max_len :\n",
    "        Maximum length of encoded streams\n",
    "    bos_token_id :\n",
    "        Beginning of sentence token\n",
    "    eos_token_id :\n",
    "        End of sentence token\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ids :\n",
    "        Encoded stream of text\n",
    "    masks :\n",
    "        1 for tokens corresponding to <s> sentiment </s></s> encoding </s>,\n",
    "        0 for tokens corresponding to padding\n",
    "    tweet :\n",
    "        Raw tweet\n",
    "    offsets :\n",
    "        Positions of raw text corresponding to each token\n",
    "    \"\"\"\n",
    "    tweet = \" \" + \" \".join(row.text.lower().split())\n",
    "    encoding = tokenizer.encode(tweet)\n",
    "    sentiment_id = tokenizer.encode(row.sentiment).ids\n",
    "    ids = (\n",
    "        [bos_token_id]\n",
    "        + sentiment_id\n",
    "        + [eos_token_id, eos_token_id]\n",
    "        + encoding.ids\n",
    "        + [eos_token_id]\n",
    "    )\n",
    "    offsets = [(0, 0)] * 4 + encoding.offsets + [(0, 0)]\n",
    "\n",
    "    pad_len = max_len - len(ids)\n",
    "    if pad_len > 0:\n",
    "        ids += [1] * pad_len\n",
    "        offsets += [(0, 0)] * pad_len\n",
    "\n",
    "    ids = torch.tensor(ids)\n",
    "    masks = torch.where(ids != 1, torch.tensor(1), torch.tensor(0))\n",
    "    offsets = torch.tensor(offsets)\n",
    "\n",
    "    assert ids.shape == torch.Size([96])\n",
    "    assert masks.shape == torch.Size([96])\n",
    "    assert offsets.shape == torch.Size([96, 2])\n",
    "\n",
    "    return ids, masks, tweet, offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_target_idx(row: pd.Series, tweet: str, offsets: torch.Tensor) -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Get the start and end tokens corresponding to the target.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    row\n",
    "        Raw data\n",
    "    tweet\n",
    "        Lowercase tweet with leading space\n",
    "    offsets\n",
    "        Positions of raw text corresponding to each token\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        First token to contain a character from selected text\n",
    "    int\n",
    "        Last token to contain a character from selected text\n",
    "    \"\"\"\n",
    "    assert offsets.shape == torch.Size([96, 2])\n",
    "\n",
    "    selected_text = \" \" + \" \".join(row.selected_text.lower().split())\n",
    "\n",
    "    len_st = len(selected_text) - 1\n",
    "    idx0 = None\n",
    "    idx1 = None\n",
    "\n",
    "    # First the starting and ending index of the selected text within the tweet.\n",
    "    for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n",
    "        if \" \" + tweet[ind : ind + len_st] == selected_text:\n",
    "            idx0 = ind\n",
    "            idx1 = ind + len_st\n",
    "            break\n",
    "\n",
    "    # Character targets: 0 outside of selected text, 1 inside it.\n",
    "    char_targets = [0] * len(tweet)\n",
    "    if idx0 is not None and idx1 is not None:\n",
    "        for ct in range(idx0, idx1):\n",
    "            char_targets[ct] = 1\n",
    "\n",
    "    # If any token contains any character from the selected text, then set\n",
    "    # the target value for that token to be 1.\n",
    "    target_idx: List[int] = []\n",
    "    for j, (offset1, offset2) in enumerate(offsets):\n",
    "        left = offset1.item()\n",
    "        right = offset2.item()\n",
    "        assert isinstance(left, int)\n",
    "        assert isinstance(right, int)\n",
    "        if sum(char_targets[left:right]) > 0:\n",
    "            target_idx.append(j)\n",
    "\n",
    "    start_idx = target_idx[0]\n",
    "    end_idx = target_idx[-1]\n",
    "\n",
    "    return start_idx, end_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_row(\n",
    "    row: pd.Series,\n",
    "    tokenizer: tokenizers.ByteLevelBPETokenizer,\n",
    "    max_len: int,\n",
    "    bos_token_id: int,\n",
    "    eos_token_id: int,\n",
    ") -> Dict[str, Union[torch.Tensor, str, int]]:\n",
    "    \"\"\"\n",
    "    Process row so it can be fed into Roberta.\n",
    "\n",
    "    Processed row contains:\n",
    "    - ids (tokenised input)\n",
    "    - masks (1 outside padding)\n",
    "    - tweet (lowercase tweet with leading whitespace)\n",
    "    - offsets (mapping of tokens to tweet)\n",
    "\n",
    "    If training, it also contains:\n",
    "    - start_idx (first token containing part of selected text)\n",
    "    - end_idx (last token containing part of selected text)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    row :\n",
    "        Raw data\n",
    "    tokenizer :\n",
    "        Converts text to stream of tokens\n",
    "    max_len :\n",
    "        Maximum length of encoded streams\n",
    "    bos_token_id :\n",
    "        Beginning of sentence token\n",
    "    eos_token_id :\n",
    "        End of sentence token\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dictionary that can be processed by training loop.\n",
    "    \"\"\"\n",
    "    data: Dict[str, Union[torch.Tensor, str, int]] = {}\n",
    "\n",
    "    ids, masks, tweet, offsets = _get_input_data(\n",
    "        row, tokenizer, max_len, bos_token_id, eos_token_id\n",
    "    )\n",
    "    data.update({\"ids\": ids})\n",
    "    data[\"masks\"] = masks\n",
    "    data[\"tweet\"] = tweet\n",
    "    data[\"offsets\"] = offsets\n",
    "\n",
    "    if \"selected_text\" in row:\n",
    "        start_idx, end_idx = _get_target_idx(row, tweet, offsets)\n",
    "        data[\"start_idx\"] = start_idx\n",
    "        data[\"end_idx\"] = end_idx\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Processed rows that can be used by PyTorch for training / inference.\n",
    "\n",
    "    Processed rows contain:\n",
    "    - ids (tokenised input)\n",
    "    - masks (1 outside padding)\n",
    "    - tweet (lowercase tweet with leading whitespace)\n",
    "    - offsets (mapping of tokens to tweet)\n",
    "\n",
    "    If training, they also contain:\n",
    "    - start_idx (first token containing part of selected text)\n",
    "    - end_idx (last token containing part of selected text)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame, max_len: int = 96) -> None:\n",
    "        \"\"\"\n",
    "        Initialise TweetDataset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df\n",
    "            Raw DataFrame (e.g. training, validation, testing).\n",
    "        max_len\n",
    "            Maximum number of tokens with which to encode tweets.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.max_len = max_len\n",
    "        self.labeled = \"selected_text\" in df\n",
    "\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(f\"{HOME_DIR}/input/roberta-base\")\n",
    "        tokenizer.save_vocabulary(\".\")\n",
    "        self.bos_token_id = tokenizer.bos_token_id\n",
    "        self.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "        # vocab.json and merges.txt come from save_vocabulary above\n",
    "        self.tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "            vocab_file=\"./vocab.json\",\n",
    "            merges_file=\"./merges.txt\",\n",
    "            lowercase=True,\n",
    "            add_prefix_space=True,\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, index: int) -> Dict[str, Union[torch.Tensor, str, int]]:\n",
    "        \"\"\"\n",
    "        Return encoded version of index-th element of dataset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        index\n",
    "            Which element of the dataset to get.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Processed row of data.\n",
    "        \"\"\"\n",
    "        row = self.df.iloc[index]\n",
    "        data = _process_row(\n",
    "            row, self.tokenizer, self.max_len, self.bos_token_id, self.eos_token_id\n",
    "        )\n",
    "        return data\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Get number of rows in dataset.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "        \"\"\"\n",
    "        return len(self.df)\n",
    "\n",
    "\n",
    "def get_train_val_loaders(\n",
    "    df: pd.DataFrame, train_idx: np.array, val_idx: np.array, batch_size: int = 8\n",
    ") -> Dict[str, torch.utils.data.DataLoader]:\n",
    "    \"\"\"\n",
    "    Get train and validation data loaders.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df\n",
    "        Entire training dataset (will be split into train and val).\n",
    "    train_idx\n",
    "        Indices of train data.\n",
    "    val_idx\n",
    "        Indices of val data.\n",
    "    batch_size\n",
    "        Number of rows to include in each batch.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Keys are `train` and `val`, values are respective dataloaders.\n",
    "    \"\"\"\n",
    "    train_df = df.iloc[train_idx]\n",
    "    val_df = df.iloc[val_idx]\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        TweetDataset(train_df),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        TweetDataset(val_df), batch_size=batch_size, shuffle=False, num_workers=2,\n",
    "    )\n",
    "\n",
    "    dataloaders_dict = {\"train\": train_loader, \"val\": val_loader}\n",
    "\n",
    "    return dataloaders_dict\n",
    "\n",
    "\n",
    "def get_test_loader(df: pd.DataFrame, batch_size: int = 32) -> torch.utils.data.DataLoader:\n",
    "    \"\"\"\n",
    "    Get test data loader.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df\n",
    "        Entire testing dataset.\n",
    "    batch_size\n",
    "        Number of rows to include in each batch.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataLoader\n",
    "    \"\"\"\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        TweetDataset(df), batch_size=batch_size, shuffle=False, num_workers=2\n",
    "    )\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Model for Tweet dataset.\n",
    "\n",
    "    Structure is:\n",
    "    - Pretrained Roberta base\n",
    "    - dropout\n",
    "    - fully connected layer with 2 outputs\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"Initialise TweetModel.\"\"\"\n",
    "        super(TweetModel, self).__init__()\n",
    "\n",
    "        self.roberta = RobertaModel.from_pretrained(\n",
    "            \"../input/roberta-base\", output_hidden_states=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(self.roberta.config.hidden_size, 2)\n",
    "        nn.init.normal_(self.fc.weight, std=0.02)\n",
    "        nn.init.normal_(self.fc.bias, 0)\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids: torch.Tensor, attention_mask: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Compute forward-pass when training model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_ids\n",
    "            Batch of encoded tweets\n",
    "        attention mask\n",
    "            Batch of attention masks, telling us which tokens are outside the padding.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        start_logits\n",
    "            Predictions for where the selected text should start\n",
    "        end_logits\n",
    "            Predictions for where the selected text should end\n",
    "        \"\"\"\n",
    "        assert input_ids.shape[1:] == torch.Size([96])\n",
    "        assert attention_mask.shape[1:] == torch.Size([96])\n",
    "\n",
    "        hs: Tuple[torch.Tensor, ...]\n",
    "        _, _, hs = self.roberta(input_ids, attention_mask)\n",
    "\n",
    "        assert len(hs) == 13\n",
    "        assert all(i.shape[1:] == torch.Size([96, 768]) for i in hs)\n",
    "\n",
    "        x = torch.stack([hs[-1], hs[-2], hs[-3], hs[-4]])\n",
    "        assert x.shape[0] == 4\n",
    "        assert x.shape[2:] == torch.Size([96, 768])\n",
    "\n",
    "        x = torch.mean(x, 0)\n",
    "        assert x.shape[1:] == torch.Size([96, 768])\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        assert x.shape[1:] == torch.Size([96, 2])\n",
    "\n",
    "        start_logits, end_logits = x.split(1, dim=-1)\n",
    "        assert all(i.shape[1:] == torch.Size([96, 1]) for i in [start_logits, end_logits])\n",
    "\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "        assert all(i.shape[1:] == torch.Size([96]) for i in [start_logits, end_logits])\n",
    "\n",
    "        return start_logits, end_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(\n",
    "    start_logits: torch.Tensor,\n",
    "    end_logits: torch.Tensor,\n",
    "    start_positions: torch.Tensor,\n",
    "    end_positions: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calculate cross-entropy losses for start and end token predictions and sum them.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    start_logits\n",
    "        Logits for prediction of starting token.\n",
    "    end_logits\n",
    "        Logits for prediction of ending token.\n",
    "    start_positions\n",
    "        Ground truth starting token.\n",
    "    end_positions\n",
    "        Ground truth ending token.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        start loss + end loss\n",
    "    \"\"\"\n",
    "    assert start_logits.shape[1:] == torch.Size([96])\n",
    "    assert end_logits.shape[1:] == torch.Size([96])\n",
    "    assert start_positions.shape[1:] == torch.Size([])\n",
    "    assert end_positions.shape[1:] == torch.Size([])\n",
    "\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    start_loss = ce_loss(start_logits, start_positions)\n",
    "    assert start_loss.shape == torch.Size([])\n",
    "\n",
    "    end_loss = ce_loss(end_logits, end_positions)\n",
    "    assert end_loss.shape == torch.Size([])\n",
    "\n",
    "    total_loss = start_loss + end_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_selected_text(text: str, start_idx: int, end_idx: int, offsets: np.array) -> str:\n",
    "    \"\"\"\n",
    "    Extract selected text from tweet.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text\n",
    "        Lowercase tweet with leading whitespace\n",
    "    start_idx\n",
    "        Token where selected text starts\n",
    "    end_idx\n",
    "        Token where selected text ends\n",
    "    offsets\n",
    "        Positions of text corresponding to each token\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Selected text with leading blank space\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> text = \" i love python\"\n",
    "    >>> start_idx = 2\n",
    "    >>> end_idx = 2\n",
    "    >>> offsets = np.array([[0, 2], [2, 7], [7, 14]] + [[0, 0]] * 93)\n",
    "    >>> get_selected_text(text, start_idx, end_idx, offsets)\n",
    "    ' python'\n",
    "    \"\"\"\n",
    "    assert offsets.shape == (96, 2)\n",
    "\n",
    "    selected_text = \"\"\n",
    "    for ix in range(start_idx, end_idx + 1):\n",
    "        selected_text += text[offsets[ix][0] : offsets[ix][1]]\n",
    "        if (ix + 1) < len(offsets) and offsets[ix][1] < offsets[ix + 1][0]:\n",
    "            selected_text += \" \"\n",
    "    return selected_text\n",
    "\n",
    "\n",
    "def jaccard(str1: str, str2: str) -> float:\n",
    "    \"\"\"\n",
    "    Compute Jaccard Score of two strings.\n",
    "\n",
    "    This is given by the intersection over the union.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    str1\n",
    "        First string.\n",
    "    str2\n",
    "        Second string.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Jaccard score.\n",
    "    \"\"\"\n",
    "    a = set(str1.lower().split())\n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "\n",
    "def compute_jaccard_score(\n",
    "    text: str,\n",
    "    start_idx: int,\n",
    "    end_idx: int,\n",
    "    start_logits: np.array,\n",
    "    end_logits: np.array,\n",
    "    offsets: np.array,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute Jaccard score for one prediction.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text\n",
    "        Lowercase tweet with leading whitespace.\n",
    "    start_idx\n",
    "        Ground truth starting token for selected text.\n",
    "    end_idx\n",
    "        Ground truth ending token for selected text.\n",
    "    start_logits\n",
    "        Predicted logits for start idx.\n",
    "    end_logits\n",
    "        Predicted logits for end idx.\n",
    "    offsets\n",
    "        Positions of text corresponding to tokens.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Jaccard score.\n",
    "    \"\"\"\n",
    "    assert start_logits.shape == (96,)\n",
    "    assert end_logits.shape == (96,)\n",
    "    assert offsets.shape == (96, 2)\n",
    "\n",
    "    start_pred = np.argmax(start_logits)\n",
    "    end_pred = np.argmax(end_logits)\n",
    "\n",
    "    if start_pred > end_pred:\n",
    "        pred = text\n",
    "    else:\n",
    "        pred = get_selected_text(text, start_pred, end_pred, offsets)\n",
    "\n",
    "    true = get_selected_text(text, start_idx, end_idx, offsets)\n",
    "\n",
    "    return jaccard(true, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_through_data_loader(\n",
    "    model: TweetModel,\n",
    "    dataloaders_dict: Dict[str, torch.utils.data.DataLoader],\n",
    "    optimizer: optim.AdamW,\n",
    "    num_epochs: int,\n",
    "    epoch: int,\n",
    "    phase: str,\n",
    ") -> TweetModel:\n",
    "    \"\"\"\n",
    "    Train on training data or make prediction for validation data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model\n",
    "        The NLP model, which may already have been partially trained on a previous\n",
    "        epoch.\n",
    "    dataloaders_dict\n",
    "        Train and val dataloaders\n",
    "    optimizer\n",
    "        Adapts learning rate for each weight.\n",
    "    num_epochs\n",
    "        Total number of epochs we're training for\n",
    "    epoch\n",
    "        Current epoch\n",
    "    phase\n",
    "        Train or val\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model\n",
    "    \"\"\"\n",
    "    assert phase in [\"train\", \"val\"]\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    epoch_jaccard = 0.0\n",
    "\n",
    "    for data in dataloaders_dict[phase]:\n",
    "        ids = data[\"ids\"].cuda()\n",
    "        masks = data[\"masks\"].cuda()\n",
    "        tweet = data[\"tweet\"]\n",
    "        offsets = data[\"offsets\"].numpy()\n",
    "        start_idx = data[\"start_idx\"].cuda()\n",
    "        end_idx = data[\"end_idx\"].cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.set_grad_enabled(phase == \"train\"):\n",
    "\n",
    "            start_logits, end_logits = model(ids, masks)\n",
    "\n",
    "            loss = loss_fn(start_logits, end_logits, start_idx, end_idx)\n",
    "\n",
    "            if phase == \"train\":\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item() * len(ids)\n",
    "\n",
    "            start_idx = start_idx.cpu().detach().numpy()\n",
    "            end_idx = end_idx.cpu().detach().numpy()\n",
    "            start_logits = torch.softmax(start_logits, dim=1).cpu().detach().numpy()\n",
    "            end_logits = torch.softmax(end_logits, dim=1).cpu().detach().numpy()\n",
    "\n",
    "            for i in range(len(ids)):\n",
    "                jaccard_score = compute_jaccard_score(\n",
    "                    tweet[i],\n",
    "                    start_idx[i],\n",
    "                    end_idx[i],\n",
    "                    start_logits[i],\n",
    "                    end_logits[i],\n",
    "                    offsets[i],\n",
    "                )\n",
    "                epoch_jaccard += jaccard_score\n",
    "\n",
    "    epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
    "    epoch_jaccard = epoch_jaccard / len(dataloaders_dict[phase].dataset)\n",
    "\n",
    "    print(\n",
    "        \"Epoch {}/{} | {:^5} | Loss: {:.4f} | Jaccard: {:.4f}\".format(\n",
    "            epoch + 1, num_epochs, phase, epoch_loss, epoch_jaccard\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(\n",
    "    model: TweetModel,\n",
    "    dataloaders_dict: Dict[str, torch.utils.data.DataLoader],\n",
    "    optimizer: optim.AdamW,\n",
    "    num_epochs: int,\n",
    "    epoch: int,\n",
    ") -> TweetModel:\n",
    "    \"\"\"\n",
    "    Train on training data for one epoch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model\n",
    "        The NLP model, which may already have been partially trained on a previous\n",
    "        epoch.\n",
    "    dataloaders_dict\n",
    "        Train and val dataloaders\n",
    "    optimizer\n",
    "        Adapts learning rate for each weight.\n",
    "    num_epochs\n",
    "        Total number of epochs we're training for\n",
    "    epoch\n",
    "        Current epoch\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model\n",
    "        Trained for one extra epoch.\n",
    "    \"\"\"\n",
    "    # train\n",
    "    model.train()\n",
    "    model = loop_through_data_loader(\n",
    "        model, dataloaders_dict, optimizer, num_epochs, epoch, \"train\",\n",
    "    )\n",
    "\n",
    "    # evaluate\n",
    "    model.eval()\n",
    "    model = loop_through_data_loader(\n",
    "        model, dataloaders_dict, optimizer, num_epochs, epoch, \"val\"\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model: TweetModel,\n",
    "    dataloaders_dict: Dict[str, torch.utils.data.DataLoader],\n",
    "    optimizer: optim.AdamW,\n",
    "    num_epochs: int,\n",
    "    filename: str,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Train on training data for given number of epochs, save model weights.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model\n",
    "        The NLP model, which may already have been partially trained on a previous\n",
    "        epoch.\n",
    "    dataloaders_dict\n",
    "        Train and val dataloaders\n",
    "    optimizer\n",
    "        Adapts learning rate for each weight.\n",
    "    num_epochs\n",
    "        Total number of epochs we're training for\n",
    "    filename\n",
    "        Where to save model weights.\n",
    "    \"\"\"\n",
    "    model.cuda()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model = train_one_epoch(model, dataloaders_dict, optimizer, num_epochs, epoch)\n",
    "\n",
    "    torch.save(model.state_dict(), filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Fold: 1\nEpoch 1/1 | train | Loss: 9.2328 | Jaccard: 0.4743\nEpoch 1/1 |  val  | Loss: 7.6909 | Jaccard: 0.4400\nFold: 2\nEpoch 1/1 | train | Loss: 8.6484 | Jaccard: 0.3886\nEpoch 1/1 |  val  | Loss: 8.0283 | Jaccard: 0.3443\nCPU times: user 10.4 s, sys: 1.99 s, total: 12.3 s\nWall time: 11.2 s\n"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    if not RUN_LOCAL:\n",
    "        train_df = pd.read_csv(\"../input/tweet-sentiment-extraction/train.csv\")\n",
    "    else:\n",
    "        train_df = pd.read_csv(\"../input/tweet-sentiment-extraction/train.csv\", nrows=10)\n",
    "    train_df[\"text\"] = train_df[\"text\"].astype(str)\n",
    "    train_df[\"selected_text\"] = train_df[\"selected_text\"].astype(str)\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(\n",
    "        SKF.split(train_df, train_df.sentiment), start=1\n",
    "    ):\n",
    "        print(f\"Fold: {fold}\")\n",
    "\n",
    "        model = TweetModel()\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=3e-5, betas=(0.9, 0.999))\n",
    "        dataloaders_dict = get_train_val_loaders(train_df, train_idx, val_idx, BATCH_SIZE)\n",
    "\n",
    "        train_model(\n",
    "            model, dataloaders_dict, optimizer, NUM_EPOCHS, f\"roberta_fold{fold}.pth\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "CPU times: user 8.78 s, sys: 751 ms, total: 9.53 s\nWall time: 6.8 s\n"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if not RUN_LOCAL:\n",
    "        test_df = pd.read_csv(\"../input/tweet-sentiment-extraction/test.csv\")\n",
    "    else:\n",
    "        test_df = pd.read_csv(\"../input/tweet-sentiment-extraction/test.csv\", nrows=1)\n",
    "    test_df[\"text\"] = test_df[\"text\"].astype(str)\n",
    "    test_loader = get_test_loader(test_df)\n",
    "    predictions = []\n",
    "    models = []\n",
    "    for fold in range(SKF.n_splits):\n",
    "        model = TweetModel()\n",
    "        model.cuda()\n",
    "        model.load_state_dict(torch.load(f\"./roberta_fold{fold+1}.pth\"))\n",
    "        model.eval()\n",
    "        models.append(model)\n",
    "\n",
    "    for data in test_loader:\n",
    "        ids = data[\"ids\"].cuda()\n",
    "        masks = data[\"masks\"].cuda()\n",
    "        tweet = data[\"tweet\"]\n",
    "        offsets = data[\"offsets\"].numpy()\n",
    "\n",
    "        start_logits = []\n",
    "        end_logits = []\n",
    "        for model in models:\n",
    "            with torch.no_grad():\n",
    "                output = model(ids, masks)\n",
    "                start_logits.append(torch.softmax(output[0], dim=1).cpu().detach().numpy())\n",
    "                end_logits.append(torch.softmax(output[1], dim=1).cpu().detach().numpy())\n",
    "\n",
    "        start_logits = np.mean(start_logits, axis=0)\n",
    "        end_logits = np.mean(end_logits, axis=0)\n",
    "        for i in range(len(ids)):\n",
    "            start_pred = np.argmax(start_logits[i])\n",
    "            end_pred = np.argmax(end_logits[i])\n",
    "            if start_pred > end_pred:\n",
    "                pred = tweet[i]\n",
    "            else:\n",
    "                pred = get_selected_text(tweet[i], start_pred, end_pred, offsets[i])\n",
    "            predictions.append(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    if not RUN_LOCAL:\n",
    "        sub_df = pd.read_csv(\"../input/tweet-sentiment-extraction/sample_submission.csv\")\n",
    "    else:\n",
    "        sub_df = pd.read_csv(\n",
    "            \"../input/tweet-sentiment-extraction/sample_submission.csv\", nrows=1\n",
    "        )\n",
    "    sub_df[\"selected_text\"] = predictions\n",
    "    sub_df[\"selected_text\"] = sub_df[\"selected_text\"].apply(\n",
    "        lambda x: x.replace(\"!!!!\", \"!\") if len(x.split()) == 1 else x\n",
    "    )\n",
    "    sub_df[\"selected_text\"] = sub_df[\"selected_text\"].apply(\n",
    "        lambda x: x.replace(\"..\", \".\") if len(x.split()) == 1 else x\n",
    "    )\n",
    "    sub_df[\"selected_text\"] = sub_df[\"selected_text\"].apply(\n",
    "        lambda x: x.replace(\"...\", \".\") if len(x.split()) == 1 else x\n",
    "    )\n",
    "    sub_df.to_csv(\"submission.csv\", index=False)\n",
    "    sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
