{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet sentiment extraction\n",
    "\n",
    "This notebook is my attempt to clean up / refactor [this](https://www.kaggle.com/shoheiazuma/tweet-sentiment-roberta-pytorch) excellent notebook.\n",
    "\n",
    "Type-checking, linting, and testing is done via [nbQA](https://github.com/MarcoGorelli/nbQA) , e.g.\n",
    "\n",
    "```\n",
    "nbqa mypy cln-tweet-sentiment-roberta-pytorch.ipynb\n",
    "```\n",
    "\n",
    "----\n",
    "\n",
    "Disclaimer: I'm the authour of `nbQA`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tokenizers\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    from transformers import RobertaModel, RobertaTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTING = os.environ.get(\"TESTING\", None) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TESTING:\n",
    "    NUM_EPOCHS = 1\n",
    "    HOME_DIR = \"/workspaces/kaggle\"\n",
    "else:\n",
    "    NUM_EPOCHS = 3\n",
    "    HOME_DIR = \"/kaggle\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed_value)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "seed = 42\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_input_data(\n",
    "    row: pd.Series,\n",
    "    tokenizer: tokenizers.ByteLevelBPETokenizer,\n",
    "    max_len: int,\n",
    "    bos_token_id: int,\n",
    "    eos_token_id: int,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Encode input row so it can be processed by Roberta.\n",
    "\n",
    "    Encoding follows: <s> sentiment </s></s> encoding </s>\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    row :\n",
    "        Raw data\n",
    "    tokenizer :\n",
    "        Converts text to stream of tokens\n",
    "    max_len :\n",
    "        Maximum length of encoded streams\n",
    "    bos_token_id :\n",
    "        Beginning of sentence token\n",
    "    eos_token_id :\n",
    "        End of sentence token\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ids :\n",
    "        Encoded stream of text\n",
    "    masks :\n",
    "        1 for tokens corresponding to <s> sentiment </s></s> encoding </s>,\n",
    "        0 for tokens corresponding to padding\n",
    "    tweet :\n",
    "        Raw tweet\n",
    "    offsets :\n",
    "        Positions of raw text corresponding to each token\n",
    "    \"\"\"\n",
    "    tweet = \" \" + \" \".join(row.text.lower().split())\n",
    "    encoding = tokenizer.encode(tweet)\n",
    "    sentiment_id = tokenizer.encode(row.sentiment).ids\n",
    "    ids = (\n",
    "        [bos_token_id]\n",
    "        + sentiment_id\n",
    "        + [eos_token_id, eos_token_id]\n",
    "        + encoding.ids\n",
    "        + [eos_token_id]\n",
    "    )\n",
    "    offsets = [(0, 0)] * 4 + encoding.offsets + [(0, 0)]\n",
    "\n",
    "    pad_len = max_len - len(ids)\n",
    "    if pad_len > 0:\n",
    "        ids += [1] * pad_len\n",
    "        offsets += [(0, 0)] * pad_len\n",
    "\n",
    "    ids = torch.tensor(ids)\n",
    "    masks = torch.where(ids != 1, torch.tensor(1), torch.tensor(0))\n",
    "    offsets = torch.tensor(offsets)\n",
    "\n",
    "    assert ids.shape == torch.Size([96])\n",
    "    assert masks.shape == torch.Size([96])\n",
    "    assert offsets.shape == torch.Size([96, 2])\n",
    "\n",
    "    return ids, masks, tweet, offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_target_idx(row: pd.Series, tweet: str, offsets: torch.Tensor) -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Get the start and end tokens corresponding to the target.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    row\n",
    "        Raw data\n",
    "    tweet\n",
    "        Lowercase tweet with leading space\n",
    "    offsets\n",
    "        Positions of raw text corresponding to each token\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        First token to contain a character from selected text\n",
    "    int\n",
    "        Last token to contain a character from selected text\n",
    "    \"\"\"\n",
    "    assert offsets.shape == torch.Size([96, 2])\n",
    "\n",
    "    selected_text = \" \" + \" \".join(row.selected_text.lower().split())\n",
    "\n",
    "    len_st = len(selected_text) - 1\n",
    "    idx0 = None\n",
    "    idx1 = None\n",
    "\n",
    "    # First the starting and ending index of the selected text within the tweet.\n",
    "    for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n",
    "        if \" \" + tweet[ind : ind + len_st] == selected_text:\n",
    "            idx0 = ind\n",
    "            idx1 = ind + len_st\n",
    "            break\n",
    "\n",
    "    # Character targets: 0 outside of selected text, 1 inside it.\n",
    "    char_targets = [0] * len(tweet)\n",
    "    if idx0 is not None and idx1 is not None:\n",
    "        for ct in range(idx0, idx1):\n",
    "            char_targets[ct] = 1\n",
    "\n",
    "    # If any token contains any character from the selected text, then set\n",
    "    # the target value for that token to be 1.\n",
    "    target_idx: List[int] = []\n",
    "    for j, (offset1, offset2) in enumerate(offsets):\n",
    "        left = offset1.item()\n",
    "        right = offset2.item()\n",
    "        assert isinstance(left, int)\n",
    "        assert isinstance(right, int)\n",
    "        if sum(char_targets[left:right]) > 0:\n",
    "            target_idx.append(j)\n",
    "\n",
    "    start_idx = target_idx[0]\n",
    "    end_idx = target_idx[-1]\n",
    "\n",
    "    return start_idx, end_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_row(\n",
    "    row: pd.Series,\n",
    "    tokenizer: tokenizers.ByteLevelBPETokenizer,\n",
    "    max_len: int,\n",
    "    bos_token_id: int,\n",
    "    eos_token_id: int,\n",
    ") -> Dict[str, Union[torch.Tensor, str, int]]:\n",
    "    \"\"\"\n",
    "    Process row so it can be fed into Roberta.\n",
    "\n",
    "    Processed row contains:\n",
    "    - ids (tokenised input)\n",
    "    - masks (1 outside padding)\n",
    "    - tweet (lowercase tweet with leading whitespace)\n",
    "    - offsets (mapping of tokens to tweet)\n",
    "\n",
    "    If training, it also contains:\n",
    "    - start_idx (first token containing part of selected text)\n",
    "    - end_idx (last token containing part of selected text)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    row :\n",
    "        Raw data\n",
    "    tokenizer :\n",
    "        Converts text to stream of tokens\n",
    "    max_len :\n",
    "        Maximum length of encoded streams\n",
    "    bos_token_id :\n",
    "        Beginning of sentence token\n",
    "    eos_token_id :\n",
    "        End of sentence token\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dictionary that can be processed by training loop.\n",
    "    \"\"\"\n",
    "    data: Dict[str, Union[torch.Tensor, str, int]] = {}\n",
    "\n",
    "    ids, masks, tweet, offsets = _get_input_data(\n",
    "        row, tokenizer, max_len, bos_token_id, eos_token_id\n",
    "    )\n",
    "    data.update({\"ids\": ids})\n",
    "    data[\"masks\"] = masks\n",
    "    data[\"tweet\"] = tweet\n",
    "    data[\"offsets\"] = offsets\n",
    "\n",
    "    if \"selected_text\" in row:\n",
    "        start_idx, end_idx = _get_target_idx(row, tweet, offsets)\n",
    "        data[\"start_idx\"] = start_idx\n",
    "        data[\"end_idx\"] = end_idx\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "row = pd.Series(\n",
    "    {\n",
    "        \"textID\": \"cb774db0d1\",\n",
    "        \"text\": \" I`d have responded, if I were going\",\n",
    "        \"selected_text\": \"I`d have responded, if I were going\",\n",
    "        \"sentiment\": \"neutral\",\n",
    "    }\n",
    ")\n",
    "r_tokenizer = RobertaTokenizer.from_pretrained(f\"{HOME_DIR}/input/roberta-base\")\n",
    "r_tokenizer.save_vocabulary(\".\")\n",
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "    vocab_file=\"./vocab.json\",\n",
    "    merges_file=\"./merges.txt\",\n",
    "    lowercase=True,\n",
    "    add_prefix_space=True,\n",
    ")\n",
    "\n",
    "\n",
    "def test_process_row():\n",
    "    data = _process_row(row, tokenizer, 96, r_tokenizer.bos_token_id, r_tokenizer.eos_token_id)\n",
    "    data[\"tweet\"]\n",
    "    result_ids = data[\"ids\"][data[\"masks\"] == 1]\n",
    "\n",
    "    expected_ids = torch.tensor(\n",
    "        [0, 7974, 2, 2, 939, 12905, 417, 33, 2334, 6, 114, 939, 58, 164, 2]\n",
    "    )\n",
    "    torch.all(result_ids.eq(expected_ids))\n",
    "\n",
    "    result_selected_text = [\n",
    "        data[\"tweet\"][data[\"offsets\"][n][0] : data[\"offsets\"][n][1]]\n",
    "        for n in range(data[\"start_idx\"], data[\"end_idx\"] + 1)\n",
    "    ]\n",
    "    expected_selected_text = [\n",
    "        \" i\",\n",
    "        \"`\",\n",
    "        \"d\",\n",
    "        \" have\",\n",
    "        \" responded\",\n",
    "        \",\",\n",
    "        \" if\",\n",
    "        \" i\",\n",
    "        \" were\",\n",
    "        \" going\",\n",
    "    ]\n",
    "    assert result_selected_text == expected_selected_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, max_len: int = 96) -> None:\n",
    "        \"\"\"\n",
    "        Initialise TweetDataset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df\n",
    "            Raw DataFrame (e.g. training, validation, testing).\n",
    "        max_len\n",
    "            Maximum number of tokens with which to encode tweets.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.max_len = max_len\n",
    "        self.labeled = \"selected_text\" in df\n",
    "\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(f\"{HOME_DIR}/input/roberta-base\")\n",
    "        tokenizer.save_vocabulary(\".\")\n",
    "        self.bos_token_id = tokenizer.bos_token_id\n",
    "        self.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "        # vocab.json and merges.txt come from save_vocabulary above\n",
    "        self.tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "            vocab_file=\"./vocab.json\",\n",
    "            merges_file=\"./merges.txt\",\n",
    "            lowercase=True,\n",
    "            add_prefix_space=True,\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, index: int) -> Dict[str, Union[torch.Tensor, str, int]]:\n",
    "        \"\"\"\n",
    "        Return encoded version of index-th element of dataset.\n",
    "\n",
    "        Processed row contains:\n",
    "        - ids (tokenised input)\n",
    "        - masks (1 outside padding)\n",
    "        - tweet (lowercase tweet with leading whitespace)\n",
    "        - offsets (mapping of tokens to tweet)\n",
    "\n",
    "        If training, it also contains:\n",
    "        - start_idx (first token containing part of selected text)\n",
    "        - end_idx (last token containing part of selected text)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        index\n",
    "            Which element of the dataset to get.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Processed row of data.\n",
    "        \"\"\"\n",
    "        row = self.df.iloc[index]\n",
    "        data = _process_row(\n",
    "            row, self.tokenizer, self.max_len, self.bos_token_id, self.eos_token_id\n",
    "        )\n",
    "        return data\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Get number of rows in dataset.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "        \"\"\"\n",
    "        return len(self.df)\n",
    "\n",
    "\n",
    "def get_train_val_loaders(\n",
    "    df: pd.DataFrame, train_idx: np.array, val_idx: np.array, batch_size: int = 8\n",
    ") -> Dict[str, torch.utils.data.DataLoader]:\n",
    "    \"\"\"\n",
    "    Get train and validation data loaders.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df\n",
    "        Entire training dataset (will be split into train and val).\n",
    "    train_idx\n",
    "        Indices of train data.\n",
    "    val_idx\n",
    "        Indices of val data.\n",
    "    batch_size\n",
    "        Number of rows to include in each batch.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Keys are `train` and `val`, values are respective dataloaders.\n",
    "    \"\"\"\n",
    "    train_df = df.iloc[train_idx]\n",
    "    val_df = df.iloc[val_idx]\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        TweetDataset(train_df),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        TweetDataset(val_df), batch_size=batch_size, shuffle=False, num_workers=2,\n",
    "    )\n",
    "\n",
    "    dataloaders_dict = {\"train\": train_loader, \"val\": val_loader}\n",
    "\n",
    "    return dataloaders_dict\n",
    "\n",
    "\n",
    "def get_test_loader(df: pd.DataFrame, batch_size: int = 32) -> torch.utils.data.DataLoader:\n",
    "    \"\"\"\n",
    "    Get test data loader.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df\n",
    "        Entire testing dataset.\n",
    "    batch_size\n",
    "        Number of rows to include in each batch.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataLoader\n",
    "    \"\"\"\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        TweetDataset(df), batch_size=batch_size, shuffle=False, num_workers=2\n",
    "    )\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialise TweetModel.\n",
    "\n",
    "        Structure is:\n",
    "        - Pretrained Roberta base\n",
    "        - dropout\n",
    "        - fully connected layer with 2 outputs\n",
    "        \"\"\"\n",
    "        super(TweetModel, self).__init__()\n",
    "\n",
    "        self.roberta = RobertaModel.from_pretrained(\n",
    "            \"../input/roberta-base\", output_hidden_states=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(self.roberta.config.hidden_size, 2)\n",
    "        nn.init.normal_(self.fc.weight, std=0.02)\n",
    "        nn.init.normal_(self.fc.bias, 0)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Compute forward-pass when training model.\n",
    "        \"\"\"\n",
    "        _, _, hs = self.roberta(input_ids, attention_mask)\n",
    "\n",
    "        x = torch.stack([hs[-1], hs[-2], hs[-3], hs[-4]])\n",
    "        x = torch.mean(x, 0)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        start_logits, end_logits = x.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "\n",
    "        return start_logits, end_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(start_logits, end_logits, start_positions, end_positions):\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    start_loss = ce_loss(start_logits, start_positions)\n",
    "    end_loss = ce_loss(end_logits, end_positions)\n",
    "    total_loss = start_loss + end_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_selected_text(text, start_idx, end_idx, offsets):\n",
    "    selected_text = \"\"\n",
    "    for ix in range(start_idx, end_idx + 1):\n",
    "        selected_text += text[offsets[ix][0] : offsets[ix][1]]\n",
    "        if (ix + 1) < len(offsets) and offsets[ix][1] < offsets[ix + 1][0]:\n",
    "            selected_text += \" \"\n",
    "    return selected_text\n",
    "\n",
    "\n",
    "def jaccard(str1, str2):\n",
    "    a = set(str1.lower().split())\n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "\n",
    "def compute_jaccard_score(text, start_idx, end_idx, start_logits, end_logits, offsets):\n",
    "    start_pred = np.argmax(start_logits)\n",
    "    end_pred = np.argmax(end_logits)\n",
    "    if start_pred > end_pred:\n",
    "        pred = text\n",
    "    else:\n",
    "        pred = get_selected_text(text, start_pred, end_pred, offsets)\n",
    "\n",
    "    true = get_selected_text(text, start_idx, end_idx, offsets)\n",
    "\n",
    "    return jaccard(true, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_through_data_loader(\n",
    "    model, dataloaders_dict, criterion, optimizer, num_epochs, epoch, phase\n",
    "):\n",
    "    epoch_loss = 0.0\n",
    "    epoch_jaccard = 0.0\n",
    "\n",
    "    for data in dataloaders_dict[phase]:\n",
    "        ids = data[\"ids\"].cuda()\n",
    "        masks = data[\"masks\"].cuda()\n",
    "        tweet = data[\"tweet\"]\n",
    "        offsets = data[\"offsets\"].numpy()\n",
    "        start_idx = data[\"start_idx\"].cuda()\n",
    "        end_idx = data[\"end_idx\"].cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.set_grad_enabled(phase == \"train\"):\n",
    "\n",
    "            start_logits, end_logits = model(ids, masks)\n",
    "\n",
    "            loss = criterion(start_logits, end_logits, start_idx, end_idx)\n",
    "\n",
    "            if phase == \"train\":\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item() * len(ids)\n",
    "\n",
    "            start_idx = start_idx.cpu().detach().numpy()\n",
    "            end_idx = end_idx.cpu().detach().numpy()\n",
    "            start_logits = torch.softmax(start_logits, dim=1).cpu().detach().numpy()\n",
    "            end_logits = torch.softmax(end_logits, dim=1).cpu().detach().numpy()\n",
    "\n",
    "            for i in range(len(ids)):\n",
    "                jaccard_score = compute_jaccard_score(\n",
    "                    tweet[i],\n",
    "                    start_idx[i],\n",
    "                    end_idx[i],\n",
    "                    start_logits[i],\n",
    "                    end_logits[i],\n",
    "                    offsets[i],\n",
    "                )\n",
    "                epoch_jaccard += jaccard_score\n",
    "\n",
    "    epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
    "    epoch_jaccard = epoch_jaccard / len(dataloaders_dict[phase].dataset)\n",
    "\n",
    "    print(\n",
    "        \"Epoch {}/{} | {:^5} | Loss: {:.4f} | Jaccard: {:.4f}\".format(\n",
    "            epoch + 1, num_epochs, phase, epoch_loss, epoch_jaccard\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloaders_dict, criterion, optimizer, num_epochs, epoch):\n",
    "    # train\n",
    "    model.train()\n",
    "    model = loop_through_data_loader(\n",
    "        model, dataloaders_dict, criterion, optimizer, num_epochs, epoch, \"train\",\n",
    "    )\n",
    "\n",
    "    # evaluate\n",
    "    model.eval()\n",
    "    model = loop_through_data_loader(\n",
    "        model, dataloaders_dict, criterion, optimizer, num_epochs, epoch, \"val\"\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders_dict, criterion, optimizer, num_epochs, filename):\n",
    "    model.cuda()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model = train_one_epoch(\n",
    "            model, dataloaders_dict, criterion, optimizer, num_epochs, epoch\n",
    "        )\n",
    "\n",
    "    torch.save(model.state_dict(), filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "CPU times: user 8 µs, sys: 0 ns, total: 8 µs\nWall time: 12.2 µs\n"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    train_df = pd.read_csv(\"../input/tweet-sentiment-extraction/train.csv\")\n",
    "    train_df[\"text\"] = train_df[\"text\"].astype(str)\n",
    "    train_df[\"selected_text\"] = train_df[\"selected_text\"].astype(str)\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(\n",
    "        skf.split(train_df, train_df.sentiment), start=1\n",
    "    ):\n",
    "        print(f\"Fold: {fold}\")\n",
    "\n",
    "        model = TweetModel()\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=3e-5, betas=(0.9, 0.999))\n",
    "        criterion = loss_fn\n",
    "        dataloaders_dict = get_train_val_loaders(train_df, train_idx, val_idx, batch_size)\n",
    "\n",
    "        train_model(\n",
    "            model,\n",
    "            dataloaders_dict,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            NUM_EPOCHS,\n",
    "            f\"roberta_fold{fold}.pth\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "CPU times: user 13 µs, sys: 0 ns, total: 13 µs\nWall time: 17.2 µs\n"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_df = pd.read_csv(\"../input/tweet-sentiment-extraction/test.csv\")\n",
    "    test_df[\"text\"] = test_df[\"text\"].astype(str)\n",
    "    test_loader = get_test_loader(test_df)\n",
    "    predictions = []\n",
    "    models = []\n",
    "    for fold in range(skf.n_splits):\n",
    "        model = TweetModel()\n",
    "        model.cuda()\n",
    "        model.load_state_dict(torch.load(f\"roberta_fold{fold+1}.pth\"))\n",
    "        model.eval()\n",
    "        models.append(model)\n",
    "\n",
    "    for data in test_loader:\n",
    "        ids = data[\"ids\"].cuda()\n",
    "        masks = data[\"masks\"].cuda()\n",
    "        tweet = data[\"tweet\"]\n",
    "        offsets = data[\"offsets\"].numpy()\n",
    "\n",
    "        start_logits = []\n",
    "        end_logits = []\n",
    "        for model in models:\n",
    "            with torch.no_grad():\n",
    "                output = model(ids, masks)\n",
    "                start_logits.append(torch.softmax(output[0], dim=1).cpu().detach().numpy())\n",
    "                end_logits.append(torch.softmax(output[1], dim=1).cpu().detach().numpy())\n",
    "\n",
    "        start_logits = np.mean(start_logits, axis=0)\n",
    "        end_logits = np.mean(end_logits, axis=0)\n",
    "        for i in range(len(ids)):\n",
    "            start_pred = np.argmax(start_logits[i])\n",
    "            end_pred = np.argmax(end_logits[i])\n",
    "            if start_pred > end_pred:\n",
    "                pred = tweet[i]\n",
    "            else:\n",
    "                pred = get_selected_text(tweet[i], start_pred, end_pred, offsets[i])\n",
    "            predictions.append(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    sub_df = pd.read_csv(\"../input/tweet-sentiment-extraction/sample_submission.csv\")\n",
    "    sub_df[\"selected_text\"] = predictions\n",
    "    sub_df[\"selected_text\"] = sub_df[\"selected_text\"].apply(\n",
    "        lambda x: x.replace(\"!!!!\", \"!\") if len(x.split()) == 1 else x\n",
    "    )\n",
    "    sub_df[\"selected_text\"] = sub_df[\"selected_text\"].apply(\n",
    "        lambda x: x.replace(\"..\", \".\") if len(x.split()) == 1 else x\n",
    "    )\n",
    "    sub_df[\"selected_text\"] = sub_df[\"selected_text\"].apply(\n",
    "        lambda x: x.replace(\"...\", \".\") if len(x.split()) == 1 else x\n",
    "    )\n",
    "    sub_df.to_csv(\"submission.csv\", index=False)\n",
    "    sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
